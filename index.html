<!DOCTYPE html>
<html lang="default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Master student in McGill University">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhenyuan Ma">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Zhenyuan Ma">
<meta property="og:description" content="Master student in McGill University">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zhenyuan Ma">
<meta name="twitter:description" content="Master student in McGill University">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Zhenyuan Ma</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zhenyuan Ma</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right"></div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-links">

    <a href="/links/" rel="section"><i class="fa fa-fw fa-sitemap"></i>links</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="default">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/07/Paper List - Traffic Signal Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhenyuan Ma">
      <meta itemprop="description" content="Master student in McGill University 
">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhenyuan Ma">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/Paper List - Traffic Signal Control/" class="post-title-link" itemprop="url">Paper List - Traffic Signal Control</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-07 14:29:11" itemprop="dateCreated datePublished" datetime="2020-02-07T14:29:11-05:00">2020-02-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-17 20:39:54" itemprop="dateModified" datetime="2020-02-17T20:39:54-05:00">2020-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/Paper-List/" itemprop="url" rel="index">
                    <span itemprop="name">Paper List</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="paper-list---traffic-signal-control">Paper List - Traffic Signal Control</h2>
<p>This is a list of papers about traffic signal control in reinforcement learning, especially for some special traffic situation.</p>
<hr>
<ul>
<li><p><a href="https://doi.org/10.1016/j.engappai.2019.103415" target="_blank" rel="noopener">Traffic-signal control reinforcement learning approach for continuous-time Markov games</a>: used a three-way intersection as an example, RL based</p></li>
<li><p><a href="https://journals.sagepub.com/doi/pdf/10.1177/1687814016641292" target="_blank" rel="noopener">Traffic design and signal timing of staggered intersections based on a sorting strategy</a>: an LR-type staggered intersection of Shenhua Road and Zijinghua Road in the city of Hangzhou, China, not RL based</p></li>
<li><p><a href="https://doi.org/10.1016/j.procs.2014.05.427" target="_blank" rel="noopener">Constrained Dynamic Control of Traffic Junctions</a>: provide a competing real-time adaptive control strategy which through the use of dynamic programming, an T-junction example</p></li>
<li><p><a href="https://pdfs.semanticscholar.org/2ffa/76209108a63aeef9df04188493db56badc79.pdf" target="_blank" rel="noopener">Decentralised reinforcement learning for ramp metering and variable speed limits on highways</a>: RL based traffic flow control on highways</p></li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5708862" target="_blank" rel="noopener">Multi-Agent Inverse Reinforcement Learning</a>: RL based, simulation a traffic signal domain with 4 intersection near highway</p></li>
<li><p><a href="https://doi.org/10.1016/j.engappai.2016.01.001" target="_blank" rel="noopener">Traffic flow optimization: A reinforcement learning approach</a>: traffic flow optimization on highway example, RL based</p></li>
<li><p><a href="https://doi.org/10.1016/j.engappai.2012.02.013" target="_blank" rel="noopener">An automated signalized junction controller that learns strategies by temporal difference reinforcement learning</a>: examples of T junction and LR intersection, RL based</p></li>
<li><p><a href="https://doi.org/10.1016/j.procs.2015.05.172" target="_blank" rel="noopener">Parallel Reinforcement Learning for Traffic Signal Control</a>, <a href="https://pdfs.semanticscholar.org/f988/4c5a132568dba4817c2d772a1176a9d703a6.pdf" target="_blank" rel="noopener">Parallel Reinforcement Learning with State Action Space Partitioning</a>: T junction example, RL based</p></li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-030-29894-4_28" target="_blank" rel="noopener">Urban Traffic Control Using Distributed Multi-agent Deep Reinforcement Learning</a>: RL based approach for multi agent traffic control with multi-type intersection</p></li>
</ul>
<hr>
<p>More to be added ...</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="default">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/07/Introduction to Time Series Analysis - 04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhenyuan Ma">
      <meta itemprop="description" content="Master student in McGill University 
">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhenyuan Ma">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/Introduction to Time Series Analysis - 04/" class="post-title-link" itemprop="url">Introduction to Time Series Analysis - 04</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-07 14:29:11" itemprop="dateCreated datePublished" datetime="2020-02-07T14:29:11-05:00">2020-02-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-16 11:55:14" itemprop="dateModified" datetime="2020-02-16T11:55:14-05:00">2020-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/Course-Note/" itemprop="url" rel="index">
                    <span itemprop="name">Course Note</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="introduction-to-time-series-analysis---04">Introduction to Time Series Analysis - 04</h2>
<p>This note is for course MATH 545 at McGill University.</p>
<hr>
<!--Lecture 10-->
<h5 id="estimation-of-mean-mu-and-auto-covariance-function-rhoh-for-stationary-x_t-when-ex_tmu"><strong>Estimation of mean <span class="math inline">\(\mu\)</span> and auto-covariance function <span class="math inline">\(\rho(h)\)</span> for stationary <span class="math inline">\(\{X_t\}\)</span> when <span class="math inline">\(E[X_t]=\mu\)</span></strong></h5>
<p><span class="math inline">\(\hat{\mu}=\bar{X}_{n}=\frac{X_{1}+\cdots+X_{n}}{n}\)</span></p>
<p><span class="math inline">\(E\left[\bar{X}_{n}\right]=\frac{E\left[X_{1}\right]+\cdots+E\left[X_{n}\right]}{n}=\mu\)</span></p>
<p><span class="math inline">\(E\left[\left(\bar{X}_{n}-\mu\right)^{2}\right]=\operatorname{MSE}\left(\bar{X}_{n}\right)=\operatorname{Var}\left(\bar{X}_{n}\right)\)</span></p>
<p><span class="math inline">\(\begin{aligned} \operatorname{Var}\left(\bar{X}_{n}\right) &amp;=\operatorname{Var}\left(\frac{1}{n}\left[X_{1}+\cdots +X_{n}\right]\right) \\ &amp;=\frac{1}{n^{2}} \text{Var} \left(X_{1}+\cdots+X_{n}\right) \\ &amp;=\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n}\text{Cov}\left(X_{i}, X_{j}\right) \\ &amp;=\frac{1}{h^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} \gamma(i-j) \\ &amp;=\frac{1}{n^{2}} \sum_{h=-n}^{n}(n-|h|) \gamma(h) \\ &amp;=\frac{1}{n} \sum_{h=-n}^{n}\left(1-\frac{|h|}{n}\right) \gamma(h) \end{aligned}\)</span></p>
<p>(Note that here <span class="math inline">\((n-|h|)\)</span> is the number of possible observed lags. And we have <span class="math inline">\(0\leq |i-j| \leq n\)</span> which means <span class="math inline">\(n+1\)</span> values of <span class="math inline">\(|i-j|\)</span>)</p>
<p>If <span class="math inline">\(\gamma(h) \rightarrow 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>, then <span class="math inline">\(\operatorname{Var}\left(\bar{X}_{n}\right)=\operatorname{MSE}\left(\bar{X}_{n}\right) \rightarrow\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p>If <span class="math inline">\(\sum_{h=-\infty}^{\infty}|\gamma(h)|&lt;\infty\)</span>, then <span class="math inline">\(\lim _{n \rightarrow \infty} n \operatorname{Var}\left(\bar{X}_{n}\right)=\sum_{n=-\infty}^{\infty}|\gamma(h)|\)</span></p>
<p>If <span class="math inline">\(\{X_t\}\)</span> are also Gaussian, then <span class="math inline">\(\bar{X}_{n} \sim N\left(\mu, \frac{1}{n} \sum_{|h|&lt;\infty}\left(1-\frac{|h|}{n}\right) \gamma(h)\right)\)</span></p>
<p>To do testing and identical estimation <span class="math inline">\(\bar{X}_{n} \pm Z_{Q / 2} \frac{\sqrt{\Gamma}}{\sqrt{n}}\)</span>, where <span class="math inline">\(\Gamma = \sum_{|h| &lt; \infty} \gamma(h)\)</span></p>
<p>Plugin <span class="math inline">\(\hat{\Gamma}=\sum_{|h|}\hat{\gamma}(h)\)</span></p>
<p>Example: AR(1) process</p>
<p>Let <span class="math inline">\(\{X_t\}\)</span> be defined by <span class="math inline">\(X_t - \mu = \phi(X_{t-1}-\mu) + Z_t\)</span> where <span class="math inline">\(|\phi| &lt; 1\)</span> and <span class="math inline">\(\{Z_t\}\sim WN(0, \sigma^2)\)</span></p>
<p>So, <span class="math inline">\(\begin{aligned} \Gamma &amp;=\sum_{|h| &lt; \infty} \gamma(h) = \sum_{|h| &lt; \infty} \frac{\sigma^2 \phi^{|h|}}{1-\phi^2} \\ &amp;=(1+2\sum^{\infty}_{h=1} \phi^{|h|})\frac{\sigma^2}{1-\phi^2} \\ &amp;=(1+2\frac{\phi}{1-\phi})\frac{\sigma^2}{1-\phi^2} \\ &amp;=\frac{1-\phi+2\phi}{1-\phi}\frac{\sigma^2}{(1+\phi)(1-\phi)} \\ &amp;=\frac{\sigma^2}{(1-\phi)^2} \end{aligned}\)</span></p>
<p>So 95% Confidence Interval for <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math inline">\(\bar{X}_n \pm \frac{1.96}{\sqrt{n}}\frac{\sigma}{\sqrt{(1-\phi)^2}} = \bar{X}_n \pm \frac{1.96}{\sqrt{n}}\frac{\sigma}{|1-\phi|}\)</span></p>
<p><span class="math inline">\(\hat{\gamma}(h)=\frac{1}{n}\sum_{t=1}^{n-|h|}(X_{t+|h|}-\bar{X}_n)(X_t-\bar{X}_n)\)</span></p>
<p><span class="math inline">\(\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}\)</span></p>
<p>Both <u>biased</u> in finite samples, but consistent</p>
<p>$_k = $</p>
<p>For all <span class="math inline">\(k&lt;n\)</span>, <span class="math inline">\(\hat{\Gamma}_k\)</span> will be non-negative definite. Not obvious how to estimate for <span class="math inline">\(k\geq n\)</span>, even for <span class="math inline">\(k\)</span> close to <span class="math inline">\(n\)</span>, <span class="math inline">\(\hat{\Gamma}\)</span> will be unstable.</p>
<p>(Jenkin's rule &amp; theorems: need <span class="math inline">\(n=50\)</span> and <span class="math inline">\(h\leq \frac{n}{4}\)</span>)</p>
<p>In large samples without large lags, can approximate the distribution of <span class="math inline">\((\hat{\rho}(1), \cdots, \hat{\rho}(k))\)</span> by:</p>
<p><span class="math display">\[\hat{\rho} \sim MVN(\rho, \frac{1}{n}W)\]</span></p>
<p>where <span class="math inline">\(W\)</span> is a <span class="math inline">\(k\times k\)</span> covariance matrix with elements computed by a simplification of butlet's formula:</p>
<p><span class="math inline">\(w_{ij}=\sum^\infty_{k=1}\{\rho(k+i)+\rho(k-i)-2\rho(k)\rho(i)\}\times\{\rho(k+j)+\rho(k-j)-2\rho(k)\rho(j)\}\)</span></p>
<p><strong><u>Example</u></strong></p>
<p><span class="math inline">\(\{X_t\}\)</span> iid <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\rho(0)=1\)</span> and <span class="math inline">\(\rho(h)=0 \quad |h|&gt;1\)</span></p>
<p>So <span class="math inline">\(w_{ij}=\sum^\infty_{k=1}\rho(k-i)\rho(k-j)\)</span></p>
<p>We have <span class="math inline">\(w_{ii}=1\)</span> and <span class="math inline">\(w_{ij}=0\)</span>, so <span class="math inline">\(\hat{\rho}(h) \sim N(0, \frac{1}{n})\)</span> as <span class="math inline">\(W=I\)</span></p>
<p>We have an MA(1) process with <span class="math inline">\(X_t=Z_t+\theta Z_{t-1}\)</span> where <span class="math inline">\(Z_t \sim WN(0, \sigma^2)\)</span></p>
<p>Then <span class="math inline">\(X_t=\sum^\infty_{k=0}\psi_k Z_{t-k}\)</span> and</p>
<p><span class="math inline">\(\Rightarrow \psi_k = \begin{cases} 1, \text{for } k=0 \\ \theta, \text{for } k=1 \\ 0, \text{for } k\neq\{0, 1\} \end{cases}\)</span></p>
<p>So <span class="math inline">\(\gamma_X(h)=\sum^\infty_{j=-\infty} \psi_j \psi_{j-h}\sigma^2 = \begin{cases}(1+\theta^2)\sigma^2, \text{for } h=0 \\ \theta\sigma^2, \text{for } h=1 \\ 0, \text{for } h\geq 2 \end{cases}\)</span></p>
<!--Lecture 11-->
<p>So <span class="math inline">\(\rho(0) = 1\)</span> and <span class="math inline">\(\rho(\pm 1) = \frac{\gamma(1)}{\gamma(0)} = \frac{\theta}{1+\theta^2}\)</span></p>
<p>From the formula <span class="math inline">\(w_{ij}=\sum^\infty_{k=1}\{\rho(k+i)+\rho(k-i)-2\rho(k)\rho(i)\}\times\{\rho(k+j)+\rho(k-j)-2\rho(k)\rho(j)\}\)</span>, we have for <span class="math inline">\(i=j\)</span></p>
<p><span class="math inline">\(w_{ii}=\sum^\infty_{k=1}\{\rho(k+i)+\rho(k-i)-2\rho(k)\rho(i)\}^2\)</span></p>
<p><span class="math inline">\(w_{11}=(\rho(o)-2\rho(1))^2 + \rho(1)^2 = 1-3\rho(1)^2+4\rho(1)^4\)</span></p>
<p>If it is the case of MA(1), <span class="math inline">\(\hat{\rho}(1) \sim N(\frac{\theta}{1+\theta^2}, \frac{1}{n}(1-3\rho(1)^2+4\rho(1)^4))\)</span></p>
<p>For <span class="math inline">\(i&gt;1\)</span>, <span class="math inline">\(w_{ii}=\sum^\infty_{k=1}\{\rho(k+i)+\rho(k-i)-2\rho(k)\rho(i)\}^2 \\=\rho(0)^2 + \rho(1)^2 + \rho(-1)^2 = 1+2\rho(1)^2\)</span></p>
<p>(Note: because in this case, <span class="math inline">\(\rho(k+i)\)</span> is always 0, <span class="math inline">\(\rho(k)\rho(i)\)</span> is always 0, <span class="math inline">\(\rho(k-i)\)</span> is nonzero only when <span class="math inline">\(k = i, i+1, i-1\)</span>)</p>
<h5 id="prediction-forecasting"><strong>Prediction (Forecasting)</strong></h5>
<p>Goal: Find linear construction of <span class="math inline">\(X_1, \cdots, X_n\)</span> that forecasts <span class="math inline">\(X_{n+h}\)</span> with minimum MSE</p>
<p>Assume best linear predictor of <span class="math inline">\(X_{n+h}\)</span> is:</p>
<p><span class="math inline">\(P_nX_{n+h} = a_0+a_1X_n+a_2X_{n-1}+\cdots+a_nX_1\)</span></p>
<p>We want to find <span class="math inline">\(a_0, a_1, \cdots, a_n\)</span> to minimize:</p>
<p><span class="math inline">\(S(a_0, a_1, \cdots, a_n) = E[(X_{n+h} - (a_0+a_1X_n+a_2X_{n-1}+\cdots+a_nX_1))^2]\)</span></p>
<p><span class="math inline">\(S\)</span> is quadratic, bounded below by zero, and there exist at least one solution to <span class="math inline">\(\frac{\partial}{\partial a_j}S(a_0, \cdots, a_n)=0\)</span> for <span class="math inline">\(j = 0, 1, \cdots, n\)</span></p>
<p>By taking derivatives, it gives (assuming interchange <span class="math inline">\(\frac{\partial}{\partial a_j}\)</span> and <span class="math inline">\(E[\cdot]\)</span> safely)</p>
<p><span class="math display">\[[1]\begin{equation}\frac{\partial}{\partial a_0}S(a_0, \cdots, a_n)=0 \Rightarrow E[X_{n+h} - a_0 - \sum^n_{i=1}a_iX_{n+1-i}]=0 \end{equation}\]</span></p>
<p><span class="math display">\[[2]\begin{equation}\frac{\partial}{\partial a_j}S(a_0, \cdots, a_n)=0 \Rightarrow E[(X_{n+h} - a_0 - \sum^n_{i=1}a_iX_{n+1-i})X_{n+1-j}]=0 \end{equation}\]</span></p>
<p>From equation 1 we have:</p>
<p><span class="math inline">\(a_0 = E(X_{n+h}) - \sum_{i=1}^n a_i E(X_{n+1-i}) \\=\mu-\mu \sum^n_{i=1} a_i\)</span> if <span class="math inline">\(\{X_t\}\)</span> is stationary</p>
<p>Plug it into equation 2, we have:</p>
<p><span class="math inline">\(E[(X_{n+h} - \mu(1-\sum^n_{i=1} a_i) - \sum^n_{i=1}a_iX_{n+1-i})X_{n+1-j}] = 0\)</span></p>
<p><span class="math inline">\(E[(X_{n+h}-\mu)X_{n+1-j}]=E[\sum^n_{i=1}a_i(X_{n+1-i}-\mu)X_{n+1-j}]\)</span></p>
<p><span class="math inline">\(\gamma(h+j-1) = \sum^n_{i=1}a_i E[(X_{n+h}-\mu)X_{n+1-j}] = \sum^n_{i=1}a_i \gamma(i-j)\)</span></p>
<p>We have a matrix form of this formula <span class="math inline">\(\Gamma_n\underset{\sim}{a_n} = \underset{\sim}{\gamma_n}(h)\)</span> where <span class="math inline">\((\Gamma_n)_{ij} = \gamma(i-j)\)</span></p>
<p>Therefore <span class="math inline">\(P_nX_{n+h} = \mu + \sum^n_{i=1}a_i(X_{n+1-i}-\mu)\)</span> where <span class="math inline">\(\underset{\sim}{a_n}\)</span> satisfies <span class="math inline">\(\Gamma_n\underset{\sim}{a_n} = \underset{\sim}{\gamma_n}(h)\)</span>.</p>
<p>Obviously, <span class="math inline">\(E[X_{n+h} - (\mu + \sum^n_{i=1}a_i(X_{n+1-i}-\mu))] = 0\)</span></p>
<p><span class="math inline">\(\begin{align}MSE &amp;=E((X_{n+h} - P_nX_{n+h})^2) \\&amp;=E(X_{n+h} - (\mu + \sum^n_{i=1}a_i(X_{n+1-i}-\mu))^2) \\&amp;=E[((X_{n+h} - \mu) -( \sum^n_{i=1}a_i(X_{n+1-i}-\mu)))^2] \\&amp;=E((X_{n+h} - \mu)^2) -2E((X_{n+h} - \mu)(\sum^n_{i=1}a_i(X_{n+1-i}-\mu))) + E((\sum^n_{i=1}a_i(X_{n+1-i}-\mu))^2) \\&amp;=\gamma(0) - 2\sum^n_{i=1} E((X_{n+h}-\mu)(X_{n+1-i}-\mu))+\sum^n_{i=1}\sum^n_{j=1}a_iE((X_{n+1-i}-\mu)(X_{n+1-j}-\mu))a_j \\&amp;=\gamma(0) - w\sum^n_{i=1}a_i\gamma(h+i-1) + \sum^n_{i=1}\sum^n_{j=1}a_i\gamma(i-j)a_j \\&amp;=\gamma(0) - w\sum^n_{i=1}a_i\gamma(h+i-1) + \sum^n_{i=1}a_i(\sum^n_{j=1}\gamma(i-j)a_j) \\&amp;=\gamma(0) - 2\underset{\sim}{a_n}^T\underset{\sim}{\gamma_n}(h) + \underset{\sim}{a_n}^T\Gamma_n\underset{\sim}{a_n} \\&amp;=\gamma(0) - \underset{\sim}{a_n}^T\underset{\sim}{\gamma_n}(h) \end{align}\)</span></p>
<p>(Note: because <span class="math inline">\(\underset{\sim}{a_n}\)</span> satisfies <span class="math inline">\(\Gamma_n\underset{\sim}{a_n} = \underset{\sim}{\gamma_n}(h)\)</span>, so <span class="math inline">\(\underset{\sim}{a_n}^T\underset{\sim}{\gamma_n}(h) = \underset{\sim}{a_n}^T\Gamma_n\underset{\sim}{a_n}\)</span> )</p>
<!--Lecture 12-->
<p><strong><u>Example</u> AR(1)</strong></p>
<p><span class="math inline">\(X_t = \phi X_{t-1} + Z_t\)</span>, where <span class="math inline">\(|\phi|&lt;1\)</span> and <span class="math inline">\(\{Z_t\} \sim WN(0, \sigma^2)\)</span></p>
<p>We try <span class="math inline">\(a_1=\phi, a_k = 0\)</span> for <span class="math inline">\(k = 2, \cdots, n\)</span></p>
<p>A solution that works is <span class="math inline">\(\underset{\sim}{a_n} = (\phi, 0, \cdots, 0)\)</span> and <span class="math inline">\(P_nX_{n+1} = \underset{\sim}{a_n}^T\underset{\sim}{X_n} = \phi X_n\)</span></p>
<p><span class="math inline">\(E((X_{n+1} - P_nX_{n+1})^2) = \gamma(0) - \underset{\sim}{a_n}^T \underset{\sim}{\gamma}(1) \\=\frac{\sigma^2}{1-\phi^2} - \phi\gamma(1) =\frac{\sigma^2}{1-\phi^2} - \phi\frac{\sigma^2\phi}{1-\phi^2} \\= \sigma^2\)</span></p>
<p>Now let <span class="math inline">\(Y\)</span> and <span class="math inline">\(W_1, \cdots, W_n\)</span> be any random variabe with finite second moments and means <span class="math inline">\(\mu = E(Y)\)</span> and <span class="math inline">\(\mu_i = E(W_i)\)</span>, and covariance <span class="math inline">\(Cov(Y, Y), Cov(Y, W_i), Cov(W_i, W-j)\)</span></p>
<p>Let <span class="math inline">\(\underset{\sim}{W} = (W_n, \cdots, W_1)\)</span>, <span class="math inline">\(\underset{\sim}{\mu} = (\mu_n, \cdots, \mu_1)\)</span>, and <span class="math inline">\(\underset{\sim}{\gamma} = Cov(Y, \underset{\sim}{W})\)</span>, <span class="math inline">\(\Gamma = Cov(\underset{\sim}{W}, \underset{\sim}{W})\)</span> such that <span class="math inline">\(\Gamma_{ij} = Cov(W_{n+1-i}, W_{n+1-j})\)</span></p>
<p>By exactly the same methods from before, we show that the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(W\)</span> is:</p>
<p><span class="math inline">\(P(Y|W) = \mu_Y + \underset{\sim}{a}^T(\underset{\sim}{W} - \underset{\sim}{\mu_W})\)</span></p>
<p>where <span class="math inline">\(\underset{\sim}{a} = (a_1, \cdots, a_n)\)</span> is any solution of <span class="math inline">\(\Gamma\underset{\sim}{a} = \underset{\sim}{\gamma}\)</span></p>
<p>Return to AR(1), assume that we observe <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> but not <span class="math inline">\(X_2\)</span></p>
<p>Let <span class="math inline">\(Y=X_2\)</span> and <span class="math inline">\(W=(X_1, X_3)\)</span>, we have</p>
<p><span class="math inline">\(\Gamma = \begin{pmatrix} \frac{\sigma^2}{1-\phi^2} &amp; \frac{\phi^2\sigma^2}{1-\phi^2}\\ \frac{\phi^2\sigma^2}{1-\phi^2} &amp; \frac{\sigma^2}{1-\phi^2} \end{pmatrix}\)</span></p>
<p><span class="math inline">\(\gamma = \begin{pmatrix} Cov(X_2, X_1) \\ Cov(X_2, X_3) \end{pmatrix} = \begin{pmatrix} \frac{\sigma^2}{1-\phi^2}\phi \\ \frac{\sigma^2}{1-\phi^2}\phi \end{pmatrix}\)</span></p>
<p>As <span class="math inline">\(\Gamma\underset{\sim}{a} = \underset{\sim}{\gamma} \Rightarrow \begin{pmatrix} 1 &amp; \phi^2\\ \phi^2 &amp; 1 \end{pmatrix}\underset{\sim}{a} = \begin{pmatrix} \phi\\ \phi \end{pmatrix}\)</span></p>
<p>So <span class="math inline">\(\underset{\sim}{a} = \begin{pmatrix} \frac{\phi}{1+\phi^2}\\ \frac{\phi}{1+\phi^2} \end{pmatrix}\)</span></p>
<p><span class="math inline">\(P(X_2|X_1, X_3) = \frac{\phi}{1+\phi^2}X_1 + \frac{\phi}{1+\phi^2}X_3 = \frac{\phi}{1+\phi^2}(X_1+X_3) + \frac{\phi}{1+\phi^2}\cdot 0\)</span></p>
<p><span class="math inline">\(P(\cdot|W)\)</span> is a prediction operator and it has useful properties:</p>
<p>Let <span class="math inline">\(E(U^2) &lt; \infty, E(V^2) &lt; \infty, \Gamma=Cov(\underset{\sim}{W}, \underset{\sim}{W})\)</span></p>
<p>Let <span class="math inline">\(\beta, a_1, \cdots, a_n\)</span> be constants</p>
<ol type="1">
<li><span class="math inline">\(P(U|W) = E(U) + a^T(W - E(\underset{\sim}{W}))\)</span> where <span class="math inline">\(\Gamma_\underset{\sim}{a} = Cov(U, \underset{\sim}{W})\)</span></li>
<li><span class="math inline">\(E((U-P(U|\underset{\sim}{W}))\underset{\sim}{W}) = \underset{\sim}{0}\)</span> and <span class="math inline">\(E(U-P(U|\underset{\sim}{W}))=0\)</span></li>
<li><span class="math inline">\(E((U-P(U|\underset{\sim}{W}))^2) = Var(U) - \underset{\sim}{a}^TCov(U, \underset{\sim}{W})\)</span></li>
<li><span class="math inline">\(P(a_iU+a_2V+\beta|W) = a_1P(U|W) + a_2P(V|W)+\beta\)</span></li>
<li><span class="math inline">\(P(\sum_{i=1}^na_iw_i + \beta|\underset{\sim}{W}) = \sum_{i=1}^na_iw_i+\beta\)</span></li>
<li><span class="math inline">\(P(U|\underset{\sim}{W}) = E(U)\)</span> if <span class="math inline">\(Cov(U,\underset{\sim}{W})=0\)</span></li>
<li><span class="math inline">\(P(U|\underset{\sim}{W})=P(P(U|\underset{\sim}{W},\underset{\sim}{V})|\underset{\sim}{W})\)</span></li>
</ol>
<p>(Note: for property 7, <span class="math inline">\(P(U|\underset{\sim}{W}, \underset{\sim}{V})=\mu_U+a_W(W-\mu_W)+a_V(V-\mu_V)\)</span>, <span class="math inline">\(P(U|\underset{\sim}{W}) = P(\mu_U + a_W(W-\mu_W)+a_V(V-\mu_V)|W)\)</span>)</p>
<p>Assume <span class="math inline">\(\{X_t\}\)</span> is a stationary process with mean 0 and auto-covariance function <span class="math inline">\(\gamma(\cdot)\)</span>, we can solve for <span class="math inline">\(\underset{\sim}{a}\)</span> to determine <span class="math inline">\(P_nX_{n+h}\)</span> in terms of <span class="math inline">\(\{X_n, \cdots, X_1\}\)</span>.</p>
<p>However, for large <span class="math inline">\(n\)</span>, inventory <span class="math inline">\(\Gamma\)</span> is not fun!</p>
<p>Perhaps we can use linearity of <span class="math inline">\(P_n\)</span> to do recursive prediction of <span class="math inline">\(P_{n+1}X_{n+h}\)</span> from <span class="math inline">\(P_nX_{n+1}\)</span>.</p>
<p>If <span class="math inline">\(\Gamma_n\)</span> id non-singular, then <span class="math inline">\(P_nX_{n+1} = \underset{\sim}{\phi^T_n}\underset{\sim}{X} = \phi_1X_n + \cdots + \phi_nX_1\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="default">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/28/Introduction to Time Series Analysis - 03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhenyuan Ma">
      <meta itemprop="description" content="Master student in McGill University 
">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhenyuan Ma">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/28/Introduction to Time Series Analysis - 03/" class="post-title-link" itemprop="url">Introduction to Time Series Analysis - 03</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-28 14:09:08" itemprop="dateCreated datePublished" datetime="2020-01-28T14:09:08-05:00">2020-01-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-07 14:28:27" itemprop="dateModified" datetime="2020-02-07T14:28:27-05:00">2020-02-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/Course-Note/" itemprop="url" rel="index">
                    <span itemprop="name">Course Note</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="introduction-to-time-series-analysis---03">Introduction to Time Series Analysis - 03</h2>
<p>This note is for course MATH 545 at McGill University.</p>
<hr>
<!--Lecture 7-->
<h5 id="test-for-weak-stationarity"><strong>Test for weak stationarity</strong></h5>
<ol type="1">
<li><p>sample autocorrelation (for white noise only)</p>
<p>problem: so many <span class="math inline">\(h\)</span>'s</p></li>
<li><p>portmanteau test</p>
<p><span class="math inline">\(Q = \sum^h_{j=1}\hat{\rho}^2(j)\)</span></p>
<p>If <span class="math inline">\(y_{t} \stackrel{\text { iid }}{\sim} N(0, \sigma^{2})\)</span>, then <span class="math inline">\(Q \sim \chi^2_h\)</span></p>
<p><span class="math inline">\(Q_{LB}=n(n+2)\sum^h_{j=1}\frac{\hat{\rho}^2(j)}{n-j}\)</span></p></li>
<li><p>Turing points test</p>
<p><span class="math inline">\(y_i\)</span> is a turing point if <span class="math inline">\(y_i &lt; y_{i-1}, y_i&lt;y_{i+1}\)</span> or <span class="math inline">\(y_i &gt; y_{i-1}, y_i&gt;y_{i+1}\)</span></p>
<p>Let <span class="math inline">\(y_i\)</span> be a turing point. For iid sequences, let <span class="math inline">\(T\)</span> be the size of turing points.</p>
<p>What's the probability of turing point after <span class="math inline">\(t\)</span>? ANS: 2/3</p>
<p>So <span class="math inline">\(E(T)=(n-2)\frac{2}{3}\)</span>, <span class="math inline">\(V(T)=\frac{16n-29}{90}\)</span></p>
<p><span class="math inline">\(\frac{T-E(T)}{\sqrt{V(T)}} \sim N(0, 1)\)</span> for large <span class="math inline">\(n\)</span></p></li>
<li><p>sign test: count <span class="math inline">\(y_{i+1} - y_i &gt;0\)</span></p>
<p>Exact signal hypothesis test for <span class="math inline">\(H_0: p=0.5\)</span></p></li>
<li><p>Rank tests (compare ranks of <span class="math inline">\(y_t\)</span> with <span class="math inline">\(t\)</span>)</p></li>
</ol>
<p>(Note: there may be 20 minutes note that I did not take)</p>
<!--Lecture 8-->
<h5 id="prediction-mx_nex_nhx_n"><strong>Prediction: <span class="math inline">\(m(X_n)=E(X_{n+h}|X_n)\)</span></strong></h5>
<p>Show that <span class="math inline">\(E(X_{n+h}|X_n)\)</span> is the unique minimizer of <span class="math inline">\(E[(X_{n+h}-m(X_n))^2]\)</span>.</p>
<p>Assume <span class="math inline">\(\hat{m}(X_n)\)</span> minimizes <span class="math inline">\(E[(X_{n+h}-m(X_n))^2]\)</span>, then <span class="math inline">\(E[(X_{n+h}-\hat{m}(X_n))^2]\)</span> is the minimum value of MSE.</p>
<p><span class="math inline">\(E[(X_{n+h}-\hat{m}(X_n))^2]\\=E\left[\left(X_{n+1}-E\left(X_{n+h} | X_{n}\right)+E\left(X_{n+h} | X_{n}\right)-\hat{m}\left(X_{n}\right)\right)^{2}\right]\\=\left.E\left[\left(X_{n+h}-E\left(X_{n+h} | X_{n}\right)\right)^{2}\right]+2 E\left[\left(X_{n+h}-E\left(X_{n+h}\right| X_{n}\right)\right)\left(E\left(X_{n+h} | X_{n}\right)-\hat{m}\left(X_{n}\right)\right)\right]+\left.E\left[E\left(X_{n+h} | X_{n}\right)-\hat{m}\left(X_{n}\right)\right)^{2}\right]\)</span></p>
<p>Focus on the second term:</p>
<p><span class="math inline">\(2E[(X_{n+h}-E(X_{n+h}| X_{n}))(E(X_{n+h} | X_{n})-\hat{m}(X_{n}))]\\=2 E_{X_{n}} E_{X_{n+h} | X_{n}}\left[\left(X_{n+h}-E\left(X_{n+h} | X_{n}\right)\right)\left(E\left(X_{n+h} | X_{n}\right)-\hat{m}\left(X_{n}\right)\right) | X_{n}\right]\\=2 E_{x_{n}}\left[\left(E\left(X_{n+h} | X_{n}\right)-m\left(X_{n}\right)\right)\right] E_{X_{n+h} | X_{n}}\left[\left(X_{n+h}-E\left(X_{n+h} | X_{n}\right)\right)\right]\\=0\)</span></p>
<p>So <span class="math inline">\(E[(X_{n+h}-\hat{m}(X_n))^2]=\left.E\left[\left(X_{n+h}-E\left(X_{n+h} | X_{n}\right)\right)^{2}\right]+E[E\left(X_{n+h} | X_{n}\right)-\hat{m}\left(X_{n}\right)\right)^{2}]\)</span></p>
<p>and this equation is greater than 0 unless <span class="math inline">\(E\left(X_{n+h} | X_{n}\right)=\hat{m}\left(X_{n}\right)\)</span> and then this equation equal to 0.</p>
<p>But this obviously extends to conditioning on <span class="math inline">\((X_1, X_2, ..., X_n)\)</span></p>
<p>Choose a model for <span class="math inline">\(E(X_{n+h}|X_n)\)</span> and we always start from <u>linear</u></p>
<p>What is the best linear predictor for <span class="math inline">\(X_{n+h}\)</span> that is a function of <span class="math inline">\(X_n\)</span>?</p>
<p><span class="math inline">\(\begin{array}{l}{\quad E\left[\left(X_{n+h}-\left(a+b X_{n}\right)\right)^{2}\right]} \\ {=E\left(X_{n+h}^{2}\right)-2 E\left[X_{n+h}\left(a+b X_{n}\right)\right]+E\left[\left(a+b X_{n}\right)^{2}\right]} \\ {=E\left(X_{n+1}^{2}\right)-2\left(a E\left(X_{n+h}\right)+b E\left(X_{n+h} X_{n}\right)\right)+a^{2}+2 ab E\left(X_{n}\right)+b^{2} E\left(X_{n}^{2}\right)}\end{array}\)</span></p>
<p>We take the derivative</p>
<p><span class="math inline">\(\frac{\partial}{\partial a}=-2 E\left(X_{n+h}\right)+2 a+2 b E\left(X_{n}\right)\)</span></p>
<p><span class="math inline">\(\frac{\partial}{\partial b}=-2 E\left(X_{n+h} X_{n}\right)+2 a E\left(X_{n}\right)+2 b E\left(X_{n}^{2}\right)\)</span></p>
<p>Set two derivatives equal to 0, we have</p>
<p><span class="math inline">\(\hat{a}=E\left(X_{n+h}\right)-\hat{b} E\left(X_{n}\right)=\mu(n+h)-\hat{b}\mu(n)\)</span></p>
<p>Adding the equation above to this equation: <span class="math inline">\(-E\left(X_{n+h} X_{n}\right)+\hat{a} E\left(X_{n}\right)+\hat{b} E\left(X_{n}^{2}\right)=0\)</span></p>
<p>We have:</p>
<p><span class="math inline">\(-E\left(X_{n+n} X_{n}\right)+(\mu(n+h)-\hat{b} \mu(n)) E\left(X_{n}\right)+\hat{b} E\left(X_{n}^{2}\right)=0\)</span></p>
<p>and thus <span class="math inline">\(\hat{b}=\frac{E\left(X_{n+h} X_{n}\right)-\mu(n+h) \mu(n)}{E\left(X_{n}^{2}\right)-(\mu(n))^{2}}=\frac{\operatorname{Cov}\left(X_{n+h}, X_{n}\right)}{\operatorname{Var}\left(X_{n}\right)}\)</span></p>
<p>So <span class="math inline">\(\hat{a}=\mu(n+h)-\frac{\operatorname{cov}\left(X_{n+h}, X_{n}\right)}{\operatorname{Var}\left(X_{n}\right)} \mu(n)\)</span></p>
<p>and <span class="math inline">\(\hat{X}_{n+h}=\hat{a}+\hat{b} X_{n}=\mu(n+h)+\frac{\operatorname{Cov}\left(X_{n+h}, X_{n}\right)}{\operatorname{Var}\left(X_{n}\right)}\left(X_{n}-\mu(n)\right)\)</span></p>
<p>If <span class="math inline">\(\{X_t\}\)</span> is stationary, then <span class="math inline">\(\hat{X}_{n+h}=\mu+\frac{\gamma(h)}{\gamma(0)}\left(X_{n}-\mu\right)=\rho(h) X_{n}+(1-\rho(h))\mu\)</span></p>
<h5 id="properties-of-gammah"><strong>Properties of <span class="math inline">\(\gamma(h)\)</span></strong></h5>
<ol type="1">
<li><span class="math inline">\(\gamma(0)=0\)</span> (variance)</li>
<li><span class="math inline">\(|\gamma(h)| \leq \gamma(0)\)</span> (Cauchy-Schwarz inequality: <span class="math inline">\(|&lt;u, v&gt;|^{2} \leq\langle u, u\rangle \cdot\langle v \cdot v\rangle\)</span>)</li>
<li><span class="math inline">\(\gamma(h)\)</span> is even: <span class="math inline">\(\gamma(h)=\gamma(-h)\)</span></li>
<li><span class="math inline">\(\gamma(h)\)</span> is non-negative definite: <span class="math inline">\(\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i} \gamma(i-j) a_{j} \geqslant 0 \quad \forall n \in Z^{+}, \quad a \in R^{n}\)</span></li>
</ol>
<p>Even stronger property: let <span class="math inline">\(\gamma(h)\)</span> be a function defined on <span class="math inline">\(h \in Z\)</span>, <span class="math inline">\(\gamma(h)\)</span> is non-negative and even <span class="math inline">\(\Leftrightarrow\)</span> It is the auto-covariance function of some stationary sequence</p>
<h5 id="strictly-stationary-series"><strong>Strictly stationary series</strong></h5>
<p><u><strong>Def</strong></u>. <span class="math inline">\(\{X_t\}\)</span> is strictly stationary if <span class="math inline">\(\left(x_{1}, \ldots, x_{n}\right) \stackrel{d}{=} \left(x_{1}+n_{1}, \ldots, x_{n+1}\right) \quad \forall n \text { and } n\)</span></p>
<p><strong><u>Properties</u></strong></p>
<ol type="1">
<li>all elements of <span class="math inline">\(\{X_t\}\)</span> are identically distributed</li>
<li><span class="math inline">\((X_t, X_{t+h}) \stackrel{d}{=} (X_1, X_{1+h})\)</span></li>
<li>If <span class="math inline">\(E(X_t^2) &lt; \infty\)</span>, then <span class="math inline">\(\{X_t\}\)</span> is <u>weakly</u> stationary</li>
<li>weakly stationary does not imply strictly stationary</li>
<li>IID process is strictly stationary</li>
</ol>
<h5 id="how-to-make-a-stationary-sequence"><strong>How to make a stationary sequence?</strong></h5>
<p>Let <span class="math inline">\(\{Z_t\}\)</span> be an iid sequence of random variables.</p>
<p>Let <span class="math inline">\(X_t=g(Z_t, Z_{t-1}, ..., Z_{t-q})\)</span>, then <span class="math inline">\(X_t\)</span> us strictly stationary, because <span class="math inline">\(\left(z_{t+h}, \ldots, z_{t+h-q}\right) \stackrel{d}{=}\left(z_{t}, \ldots, z_{t-q}\right)\)</span></p>
<p>This sequence <span class="math inline">\(\{X_t\}\)</span> is q-dependent, i.e. <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_s\)</span> are independent if <span class="math inline">\(|t-s|&gt;q\)</span></p>
<p>Generalize to weakly stationary, say that <span class="math inline">\(\{X_t\}\)</span> is q-correlated if <span class="math inline">\(\operatorname{Cov}\left(X_{t}, X_{s}\right)=0 \quad \forall|t-s|&gt;q \text { or } \gamma(h)=0 \quad \forall|h|&gt;q\)</span></p>
<p>Every second order weakly stationary process is either a linear process or can be transformed into one by substracting a deterministic component.</p>
<p><strong><u>Def</u></strong>. <span class="math inline">\(\{X_t\}\)</span> is a linear process if <span class="math inline">\(X_{t}=\sum_{j=-\infty}^{\infty} \psi_{j}-Z_{t-j}\)</span> where <span class="math inline">\(\{Z_t\}\sim WN(0, \sigma^2)\)</span> and <span class="math inline">\(\{\psi_{j}\}\)</span> is a sequence of where <span class="math inline">\(\sum_{j=-\infty}^{\infty}\left|\psi_{j}\right|&lt;\infty\)</span></p>
<!--Lecture 9-->
<p>We can view <span class="math inline">\(\{X_t\}\)</span> in terms of Backwards shift operator <span class="math inline">\(X_t=\psi(B)Z_t\)</span> where <span class="math inline">\(\psi(B)=\sum^\infty_{j=-\infty}\psi_jB^j\)</span>. (Example: Moving average process has this property)</p>
<p><span class="math inline">\(E[|X_t|] \leq E[\sum^\infty_{j=-\infty}|Z_{t-j}\psi_j|] \\ \leq \sum^\infty_{j=-\infty}|\psi_j|E[|Z_{t-j}|] \leq \sum^\infty_{j=-\infty}|\psi_j|E[|Z_{t-j}|^2]^{1/2}\)</span></p>
<p>Let <span class="math inline">\(\{Y_t\}\)</span> be stationary process with mean 0 and auto-covariance function <span class="math inline">\(\gamma_Y(h)\)</span>. If <span class="math inline">\(\sum^\infty_{j=-\infty}|\psi_j| &lt; \infty\)</span>, then for <span class="math inline">\(X_t = \sum^\infty_{j=-\infty}\psi_j Y_{t-j}=\psi(B)Y_t\)</span>, <span class="math inline">\(X_t\)</span> is also a stationary sequence with mean 0 and auto-cvovatiance funciton <span class="math inline">\(\gamma_X(h)=\sum^\infty_{j=-\infty}\sum^\infty_{k=-\infty} \psi_j \psi_k \gamma_Y(h+k-j)\)</span> where <span class="math inline">\(Y_t = \sum^\infty_{j=-\infty} \psi_j Z_{t-j}\)</span> and <span class="math inline">\(\{Z_t\}\)</span> is a White Noise process.</p>
<p><strong><u>Proof</u></strong>. (There is a little difference in the notation of <span class="math inline">\(k\)</span> and <span class="math inline">\(j\)</span> because of different definition)</p>
<p><span class="math inline">\(\gamma_X(h) = E[X_t X_{t-h}] \\=E\left[\left(\sum_{k=-\infty}^{\infty} \psi_{k} y_{t-k}\right)\left(\sum_{j=-\infty}^{\infty} \psi_{j} y_{t-j-h}\right)\right] \\=E\left[\sum_{j=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} \psi_{k} \psi_{j}\left(y_{t-k}\right)\left(y_{t-j-h}\right)\right] \\=\sum_{j=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} \psi_{k} \psi_{j} \gamma_{Y}(h+j-k)\)</span></p>
<p>If <span class="math inline">\(\{Y_t\}\)</span> is <span class="math inline">\(WN(0, \sigma^2)\)</span> process, then <span class="math inline">\(\gamma_Y(l)=0, \forall l\neq 0\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\gamma_X(h)=\sum_{j=-\infty}^{\infty} \psi_j\psi_{j-h} \sigma^{2}\)</span></p>
<p>Example: AR(1) process</p>
<p>For <span class="math inline">\(\{X_t\}\)</span> stationary, let <span class="math inline">\(X_t=\phi X_{t-1}Z_t\)</span> where <span class="math inline">\(\{Z_t\} \sim WN(0, \sigma^2)\)</span>. <span class="math inline">\(\{X_t\}\)</span> and <span class="math inline">\(\{Z_s\}\)</span> are uncorrelated for <span class="math inline">\(s&gt;t\)</span></p>
<p>Define <span class="math inline">\(\{X_t\}\)</span> to be the solution to <span class="math inline">\(X_t - \phi X_{t-1} = Z_t\)</span></p>
<p>Consider <span class="math inline">\(X_t=\sum^\infty_{j=0}\phi^jZ_{t-j}\)</span>, we have that <span class="math inline">\(\{X_t\}\)</span> is linear with <span class="math inline">\(\psi_j=\phi^j \quad \text{for} j \geq 0\)</span>, and <span class="math inline">\(\psi_j=0 \quad \text{for} j &lt; 0\)</span>.</p>
<p>And <span class="math inline">\(\sum^\infty_{j=-\infty}|\psi_j| &lt; \infty\)</span> iff <span class="math inline">\(|\phi| &lt; 1\)</span></p>
<p>To solve <span class="math inline">\(X_{t}-\phi X_{t-1}=Z_t\)</span>:</p>
<p><span class="math inline">\(\left[\sum_{j=0}^{\infty} \phi^{j} Z_{t-j}\right]-\phi\left[\sum_{j=0}^{\infty} \phi^{j} Z_{t-1-j}\right] = Z_{t}\)</span></p>
<p><span class="math inline">\(\left[\sum_{j=0}^{\infty} \phi^{j} Z_{t-j}\right]-\left[\sum_{j=0}^{\infty} \phi^{j+1} Z_{t-(j+1)}\right] = Z_{t}\)</span></p>
<p><span class="math inline">\(\left[\sum_{j=0}^{\infty} \phi^{j} Z_{t-j}\right]-\left[\sum_{j=1}^{\infty} \phi^{j} Z_{t-j}\right] = Z_{t}\)</span></p>
<p><span class="math inline">\(Z_t=Z_t\)</span></p>
<p>Therefore, <span class="math inline">\(\{Z_t\}\)</span> is stationary <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\{X_t\}\)</span> is stationary with mean 0 and auto-covariance function <span class="math inline">\(\gamma_X(h)=\sum_{j=0}^{\infty} \phi^j\phi^{j+h} \sigma^{2}=\frac{\sigma^2 \phi^h}{1-\phi^2}\)</span></p>
<p>If <span class="math inline">\(|\phi| &gt; 1\)</span>, then no stationary sequence exists that dependent on the past</p>
<p>Let <span class="math inline">\(\Phi(B)=1-\phi B\)</span> and <span class="math inline">\(\Pi(B)=\sum^\infty_{j=0}\psi^jB^j\)</span></p>
<p>Then <span class="math inline">\(\psi(B)=\Phi(B)\Pi(B)\\=(1-\phi B)(\sum^\infty_{j=0}\psi^jB^j)\\=\sum^\infty_{j=0}\psi^jB^j - \sum^\infty_{j=0}\psi^{j+1}B^{j+1}\\=\psi^0B^0=1\)</span></p>
<p><span class="math inline">\(X_t-\phi X_{t-1} = (1-\phi B)X_t=\Phi(B)X_t\)</span></p>
<p><span class="math inline">\(\Pi(B)\Phi(B)X_t=\Pi(B)Z_t\)</span></p>
<p><span class="math inline">\(\psi(B)X_t=\Pi(B)Z_t\)</span></p>
<p><span class="math inline">\(X_t = \sum^\infty_{j=0}\phi^jB_jZ_t = \sum^\infty_{j=0}\phi^jZ_{t-j}\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="default">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/21/Introduction to Time Series Analysis - 02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhenyuan Ma">
      <meta itemprop="description" content="Master student in McGill University 
">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhenyuan Ma">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/21/Introduction to Time Series Analysis - 02/" class="post-title-link" itemprop="url">Introduction to Time Series Analysis - 02</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-21 13:49:33" itemprop="dateCreated datePublished" datetime="2020-01-21T13:49:33-05:00">2020-01-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-24 15:08:05" itemprop="dateModified" datetime="2020-01-24T15:08:05-05:00">2020-01-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/Course-Note/" itemprop="url" rel="index">
                    <span itemprop="name">Course Note</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="introduction-to-time-series-analysis---02">Introduction to Time Series Analysis - 02</h2>
<p>This note is for course MATH 545 at McGill University.</p>
<p>We recommend an R package named <a href="https://www.r-project.org/nosvn/pandoc/forecast.html" target="_blank" rel="noopener">"forecast"</a>.</p>
<hr>
<!--Lecture 4-->
<h5 id="first-order-autoregressive-process-ar1"><strong>First order Autoregressive process AR(1)</strong></h5>
<p>Assume <span class="math inline">\(\{X_t\}\)</span> is a sequence of random variables that is stationary, satisfying <span class="math inline">\(X_t = \phi X_{t-1} + Z_t\)</span>, for <span class="math inline">\(t=0, \pm 1, \pm 2, ...\)</span>, where <span class="math inline">\(\{Z_t\}\)</span> is a White Noise process of <span class="math inline">\(WN(0, \sigma^2)\)</span>, and <span class="math inline">\(\{Z_t\}\)</span> is uncorrelated with <span class="math inline">\(\{X_t\}\)</span> for <span class="math inline">\(\forall s&lt;t\)</span>, and <span class="math inline">\(\phi\)</span> is a real-valued constant.</p>
<p>(Graphical representation will be added later)</p>
<p>We have <span class="math inline">\(E(X_t) = \phi E(X_{t-1}) + E(Z_t)=\phi \mu_X + 0\)</span>.</p>
<p>By assuming <span class="math inline">\(\{X_t\}\)</span> is stationary, we need <span class="math inline">\(\mu_X = 0\)</span> (or <span class="math inline">\(\phi=1\)</span> or <span class="math inline">\(\phi=0\)</span>)</p>
<p>By construction, <span class="math inline">\((X_{t-h})X_t = (X_{t-h})(X_{t-1} + Z_t)\)</span></p>
<p>Take expectation, <span class="math inline">\(E(X_{t-h}X_t) = E(X_{t-h}(X_{t-1} + Z_t))\)</span></p>
<p><span class="math inline">\(E(X_{t-h}X_t)=\phi E(X_{t-h}X_{t-1})+E(X_{t-h}Z_{t})=\phi E(X_{t-h}X_{t-1})\)</span></p>
<p>Then we have <span class="math inline">\(\gamma_X(h) = \phi \gamma_X(h-1)=\phi[\phi \gamma_X(h-2)]=...=\phi^h \gamma_X(0)\)</span>, and <span class="math inline">\(\rho_X(h) = \frac{\gamma_X(h)}{\gamma_X(0)}=\phi^h\)</span>.</p>
<p>By symmetry and stationary, we have <span class="math inline">\(\rho_X(h)=\phi^{|h|}\)</span></p>
<p><span class="math inline">\(\gamma_X(0)=Cov(X_t, X_t) \\= E((\phi X_{t-1} + Z_t)(\phi X_{t-1} + Z_t))\\=\phi^2 E(X_{t-1}^2) + E(Z_t^2) \\= \phi^2 \gamma_X(0) + \sigma^2\)</span> (as <span class="math inline">\(X_{t-1}\)</span> and <span class="math inline">\(Z_t\)</span> are not correlated by definition)</p>
<p>Therefore, we have <span class="math inline">\(\gamma_X(0) = \frac{\sigma^2}{1-\phi^2}\)</span> with <span class="math inline">\(|\phi|&lt;1\)</span>.</p>
<h5 id="estimating-autocorrelation"><strong>Estimating Autocorrelation</strong></h5>
<p>Let <span class="math inline">\(X_1, ..., X_n\)</span> be <u>observed values</u> for a stationary sequence, and sample mean <span class="math inline">\(\bar{X}=\frac{1}{n}\sum^n_{i=1}X_i\)</span>.</p>
<p>We have covariance <span class="math inline">\(Cov(V,W)=E[(V-E(V))(W-E(W))]\)</span>, and the unbiased estimator of covariance as <span class="math inline">\(\hat{Cov}(V,W)=\frac{\sum_{i=1}^n(V_i-\bar{V})(W_i-\bar{W})}{n-1}\)</span></p>
<p><span class="math inline">\(\gamma_X(h)=Cov(X_{t+h}, X_t)\)</span></p>
<p><span class="math inline">\(\hat{\gamma}_X(h)=\frac{1}{n}\sum_{t=1}^{n-|h|}(X_{t+|h|}-\bar{X})(X_t-\bar{X})\)</span> for <span class="math inline">\(-n&lt;h&lt;n\)</span></p>
<p>The sample autocorrelation <span class="math inline">\(\hat{\rho}(h)=\frac{\hat{\gamma}_X(h)}{\gamma_X(0)}\)</span>, where <span class="math inline">\(\gamma_X(0)=\frac{1}{n}\sum^n_{t=1}(X_t-\bar{X})^2\)</span>.</p>
<h5 id="classical-decomposition-model"><strong>Classical Decomposition Model</strong></h5>
<p><span class="math inline">\(X_t = m_t + S_t + Y_t\)</span></p>
<p><span class="math inline">\(m_t\)</span> here shows "trend", <span class="math inline">\(S_t\)</span> shows seasonal patterns, and <span class="math inline">\(Y_t\)</span> is random "noise" component (so far we have 4 choices of noise: iid, white noise, MA(1), and AR(1))</p>
<p>We can remove <span class="math inline">\(m_t\)</span> and <span class="math inline">\(S_t\)</span> to estimate <span class="math inline">\(Y_t\)</span>, and we have two ways:</p>
<ol type="1">
<li>estimate trend/seasonal using a "model" (filter)</li>
<li>differencing <span class="math inline">\(\{X_t\}\)</span> to estimate trend and seasonality (filter)</li>
</ol>
<h5 id="estimate-trend"><strong>Estimate Trend</strong></h5>
<p><span class="math inline">\(X_t=m_t+Y_t, \quad t=1, ..., n\)</span>, and <span class="math inline">\(E(Y_t)=0\)</span></p>
<p>We can use Nonparametric methods, which is flexible and with fewer assumptions, but is subjective</p>
<ol type="1">
<li>Finite Moving Average Filter (to capture local trend)</li>
</ol>
<p><span class="math inline">\(W_t=\frac{1}{2q+1}\sum_{j=-q}^{q}X_{t-j} \\=\frac{1}{2q+1}\sum_{j=-q}^{q}(m_{t-j}+Y_{t-j}) \\=\frac{1}{2q+1}[\sum_{j=-q}^{q}m_{t-j}] + \frac{1}{2q+1}[\sum_{j=-q}^{q}Y_{t-j}] \\ \approx \frac{1}{2q+1}\sum_{j=-q}^{q}m_{t-j}\)</span> where <span class="math inline">\(q\)</span> is a positive integer</p>
<p>Moving Average is a linear filter where <span class="math inline">\(a_j=\begin{cases}\frac{1}{2q+1}, \text{for} |j| \leq q \\ 0, \text{otherwise}\end{cases}\)</span></p>
<p>Our goal is <span class="math inline">\(X_t-\hat{m}_t = \hat{Y}_t\)</span></p>
<!--Lecture 5-->
<ol start="2" type="1">
<li>Exponential smoothing</li>
</ol>
<p><span class="math inline">\(\hat{m}_t=\alpha X_t + (1-\alpha)\hat{m}_{t-1}\)</span></p>
<p>For <span class="math inline">\(t=1\)</span>, we have <span class="math inline">\(\hat{m}_1=X_1\)</span>. For <span class="math inline">\(t\geq 2\)</span>, <span class="math inline">\(\hat{m}_t=\sum_{j=0}^{t-2} \alpha(1-\alpha)^j X_{t-j} + (1-\alpha)^{t-1}X_1\)</span>.</p>
<ol start="3" type="1">
<li>Parametric smoothing (linear, polynomial, basic function b-spline)</li>
<li>High-frequency smoothing using Fourier Series</li>
</ol>
<h5 id="differencingfor-trend"><strong>Differencing（for trend)</strong></h5>
<p>We define the lag-1 difference as <span class="math inline">\(\nabla X_t = X_t - X_{t-1} = (1-B)X_t\)</span>, where <span class="math inline">\(B\)</span> is known as the backwards shift operator with <span class="math inline">\(BX_t = X_{t-1}\)</span></p>
<p>We can generalize <span class="math inline">\(\nabla\)</span> and <span class="math inline">\(B\)</span> to general lags by taking powers:</p>
<p><span class="math inline">\(B^j X_t = B^{j-1} (BX_t) = B^{j-1}X_{t-1} = ... = X_{t-j}\)</span></p>
<p><span class="math inline">\(X_t - X_{t-j} = (1-B^j)X_t\)</span></p>
<p>As <span class="math inline">\(\nabla^jX_t = \nabla(\nabla^{j-1}X_t)\)</span>, we have:</p>
<p><span class="math inline">\(\nabla^2X_t = \nabla(\nabla X_t) = \nabla((1-B)X_t) = (1_B)(1-B)X_t \\=(1-2B+B^2)X_t = X_t -2BX_t + B^2X_t \\=X_t-2X_{t-1}+X_{t-2}\\=(X_t-X_{t-1}) - (X_{t-1}-X_{t-2})\)</span></p>
<p>Let <span class="math inline">\(X_t = m_t+Y_t\)</span>, where <span class="math inline">\(m_t = a+bt\)</span>, we have</p>
<p><span class="math inline">\(\nabla X_t = \nabla(m_t+Y_t) = \nabla m_t + \nabla Y_t \\=m_t-m_{t-1} + Y_t -Y_{t-1} = (a+bt) + (a+b(t-1)) + Y_t-Y_{t-1} \\=b+Y_t-Y_{t-1}\)</span></p>
<p>Therefore we could say <span class="math inline">\(\nabla X_t\)</span> will be stationary if <span class="math inline">\(Y_t-Y_{t-1}\)</span> is stationary.</p>
<!--Lecture 6-->
<h5 id="estimate-seasonal-component"><strong>Estimate seasonal component</strong></h5>
<p>An example of <span class="math inline">\(d=4\)</span></p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(k=1\)</span></th>
<th><span class="math inline">\(k=2\)</span></th>
<th><span class="math inline">\(k=3\)</span></th>
<th><span class="math inline">\(k=4\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\tilde{x}_{1}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{2}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{3}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{4}\)</span></td>
<td><span class="math inline">\(\rightarrow j=0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\tilde{x}_{5}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{6}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{7}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{8}\)</span></td>
<td><span class="math inline">\(\rightarrow j=1\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\tilde{x}_{9}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{10}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{11}\)</span></td>
<td><span class="math inline">\(\tilde{x}_{12}\)</span></td>
<td><span class="math inline">\(\rightarrow j=2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\downarrow\)</span></td>
<td><span class="math inline">\(\downarrow\)</span></td>
<td><span class="math inline">\(\downarrow\)</span></td>
<td><span class="math inline">\(\downarrow\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(S_1\)</span></td>
<td><span class="math inline">\(S_2\)</span></td>
<td><span class="math inline">\(S_3\)</span></td>
<td><span class="math inline">\(S_4\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(W_k=\sum_{j=1}^{t/d-1}(X_{k+jd} - \hat{m}_{k+jd})\)</span></p>
<p><span class="math inline">\(\hat{S}_k = W_k - \frac{1}{d}\sum_{i=1}^d W_i\)</span></p>
<p>Let <span class="math inline">\(d_t = X_t - \hat{S}_t\)</span> as deseasonal data, we can reestimate the trend from <span class="math inline">\(d_t\)</span> and <span class="math inline">\(\tilde{m}_t\)</span> by <span class="math inline">\(\hat{Y}_t = X_t - \hat{S}_t - \tilde{m}_t\)</span></p>
<h5 id="differencing-for-seasonal"><strong>Differencing (for seasonal)</strong></h5>
<p><span class="math inline">\(\nabla_d X_t = X_t - X_{t-d} = (1-B^d)X_t\)</span></p>
<p>Apply this to <span class="math inline">\(X_t=m_t+S_t+Y_t\)</span>, we have:</p>
<p><span class="math inline">\(\nabla_d X_t = \nabla_d(m_t + S_t+Y_t) \\=(m_t - m_{t-d}) + (S_t - S_{t-d}) + (Y_t - Y_{t-d})\)</span></p>
<p>Therefore <span class="math inline">\(\tilde{X}_t=(m_t - m_{t-d}) + (Y_t - Y_{t-d})\)</span></p>
<p>If <span class="math inline">\(m_t=a+bt\)</span>, <span class="math inline">\(\tilde{X}_t = ((a+bt)-(a+b(t-d))) + (Y_t-Y_{t-d}) = bd+ (Y_t-Y_{t-d})\)</span></p>
<p>If <span class="math inline">\(Y_{t} \stackrel{\text { iid }}{\sim}\left(0, \sigma^{2}\right)\)</span>, we have <span class="math inline">\(\hat{p}(h) \stackrel{\text { · }}{\sim} N\left(0, \frac{1}{n}\right)\)</span>. (No proof)</p>
<p>(Recall that <span class="math inline">\(\hat{p}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)} = \frac{\sum_{i=1}^{n-|h|} (X_i-\bar{X})(X_{i+n}-\bar{X})/n}{\sum_{i=1}^n (X_i-\bar{X})^2/n}\)</span>)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="default">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/18/Introduction to Time Series Analysis - 01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhenyuan Ma">
      <meta itemprop="description" content="Master student in McGill University 
">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhenyuan Ma">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/18/Introduction to Time Series Analysis - 01/" class="post-title-link" itemprop="url">Introduction to Time Series Analysis - 01</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-18 16:51:42" itemprop="dateCreated datePublished" datetime="2020-01-18T16:51:42-05:00">2020-01-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-24 15:07:46" itemprop="dateModified" datetime="2020-01-24T15:07:46-05:00">2020-01-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/Course-Note/" itemprop="url" rel="index">
                    <span itemprop="name">Course Note</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="introduction-to-time-series-analysis---01">Introduction to Time Series Analysis - 01</h2>
<p>This note is for course MATH 545 at McGill University.</p>
<h4 id="reference-book">Reference Book</h4>
<ul>
<li>Introduction to Time Series and Forecasting (by Brockwell and Davis)</li>
<li>The Analysis of Time Series: an Introduction with R (by Chatfield and Xing)</li>
</ul>
<hr>
<!--Lecture 1-->
<h5 id="time-series"><strong>Time series</strong></h5>
<p><span class="math inline">\(\{X_t\}\)</span> is a collection of random variables where \(t\) is the index of time.</p>
<h5 id="the-process-of-dealing-with-time-series"><strong>The process of dealing with time series</strong></h5>
<ol type="1">
<li>Describe by plotting to have concise summary of data</li>
<li>Explain by probabilistic models (joint distributions)</li>
<li>Predict to attain more uncertainty</li>
</ol>
<p>Note that <span class="math inline">\(X_t\)</span> is mutual independent, so we have the joint distribution <span class="math inline">\(Pr(X_1 \leq x_1, X_2 \leq x_2, ..., X_n \leq x_n) = \prod_{i=1}^n Pr(X_i \leq x_i)\)</span>. But for most complex models we assume that <span class="math inline">\(Pr(X_1 \leq x_1, X_2 \leq x_2, ..., X_n \leq x_n) = \\ Pr(X_1 \leq x_1)Pr(X_2 \leq x_2 | X_1 \leq x_1) ... Pr(X_n \leq x_n|X_1 \leq x_1 ... X_{n-1} \leq x_{n-1})\)</span>.</p>
<h5 id="semi-parametric-model"><strong>Semi-parametric model</strong></h5>
<p>In semi-parametric models, we do not specify pdf and cdf of random variables, instead we specify <span class="math inline">\(E(X_t)\)</span> and <span class="math inline">\(Cov(X_t, X_{t+j})\)</span>.</p>
<h5 id="examples"><strong>Examples</strong></h5>
<ol type="1">
<li><strong>iid noise</strong>: let <span class="math inline">\(E(X_t)=0, \forall t\)</span> and <span class="math inline">\(Pr(X_1 \leq x_1, X_2 \leq x_2, ..., X_n \leq x_n) = \prod_{i=1}^n Pr(X_i \leq x_i) = \prod_{i=1}^n F(X_n)\)</span> where <span class="math inline">\(F(\cdot)\)</span> is cumulative distribution function.</li>
<li><strong>random walk</strong>: let <span class="math inline">\(\{X_t\}\)</span> be iid noise, and <span class="math inline">\(S_t = X_1 + X_2 + ... + X_t\)</span>. Here <span class="math inline">\(S_t\)</span> is a random walk. (Note that here <span class="math inline">\(S_t\)</span> are not independent, but <span class="math inline">\(E(S_t)=0\)</span>)</li>
</ol>
<!--Lecture 2-->
<h5 id="models-with-structures"><strong>Models with structures</strong></h5>
<p>Let <span class="math inline">\(\{Y_t\}\)</span> be a time series where <span class="math inline">\(E(Y_t)=0, \forall t\)</span>. Let <span class="math inline">\(X_t=m_t + Y_t\)</span>, where <span class="math inline">\(m_t\)</span> is a slowly changing function of time. (Note that <span class="math inline">\(Y_t\)</span> here is what makes <span class="math inline">\(E(X_t)=0\)</span>, but <span class="math inline">\(m_t\)</span> here is what makes <span class="math inline">\(E(X_t) \neq 0\)</span>)</p>
<p>Some choices for <span class="math inline">\(m_t\)</span> including linear function of <span class="math inline">\(t\)</span> and polynomial function of <span class="math inline">\(t\)</span>.</p>
<h5 id="models-with-seasonal-variation-periodicity"><strong>Models with seasonal variation (periodicity)</strong></h5>
<p>Let <span class="math inline">\(X_t = S_t + Y_t\)</span>, where <span class="math inline">\(E(Y_t)=0, \forall t\)</span> and <span class="math inline">\(S_t\)</span> is a periodic function with period <span class="math inline">\(d\)</span> (i.e. <span class="math inline">\(S_{t-d}=S_t\)</span>).</p>
<p>Common choices for <span class="math inline">\(S_t\)</span> including sum of harmonic functions <span class="math inline">\(S_t = a_0 + \sum^k_{j=1}(a_j \cos(\lambda_j t) + b_j\sin(\lambda_j t))\)</span>, where <span class="math inline">\(a_j\)</span> and <span class="math inline">\(b_j\)</span> are estimated, <span class="math inline">\(\lambda_j\)</span> are fixed frequencies.</p>
<h5 id="general-strategy-for-analysis"><strong>General strategy for analysis</strong></h5>
<ol type="1">
<li>Plot the data to
<ol type="1">
<li>identify potential signal (trend, seasonal)</li>
<li>identify possible models for the rsifual process</li>
<li>identify outliers and other weird things</li>
</ol></li>
<li>Remove the signal</li>
<li>Choose a model to fit the resifual and esitimate the dependence</li>
<li>Forecast by inventory projected residuals</li>
</ol>
<h5 id="why-we-focus-on-the-residuals-i.e.-x_t---hatm_t-x_t---hats_t"><strong>Why we focus on the residuals (i.e. <span class="math inline">\(X_t - \hat{m_t}, X_t - \hat{S_t}\)</span>)</strong></h5>
<p>Let <span class="math inline">\(W_i \overset{\text{iid}}{\sim}N(\mu, \sigma^2)\)</span>, then we have <span class="math inline">\(W_i - \mu \sim N(0, \sigma^2)\)</span>. Now we can estimate <span class="math inline">\(\mu\)</span> to remove the signal, and also we can estimate <span class="math inline">\(\sigma^2\)</span>.</p>
<h5 id="stationary-process-series"><strong>Stationary process (series)</strong></h5>
<p>Let <span class="math inline">\(\{X_s\}_{s=0, 1, ..., n}\)</span> has the same properties as <span class="math inline">\(\{X_{t+s}\}_{s=0, 1, ..., n}\)</span>. (Note that we will focus on first and second order moments). iid noise is a special case of a stationary process.</p>
<p><strong><u>Def</u></strong>. <span class="math inline">\(X_t\)</span> is weakly stationary if</p>
<ol type="1">
<li><span class="math inline">\(E(X_t)=\mu_X(t)\)</span> is independent of <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(Cov(X_r, X_s)=E((X_r-\mu_X(r))(X_s-\mu_X(s)))=\gamma_X(r,s)\)</span>, where <span class="math inline">\(\gamma_X\)</span> is the covariance function of <span class="math inline">\(X_t\)</span></li>
</ol>
<p>We require that <span class="math inline">\(\gamma_X(t+h, t)\)</span> is independent of <span class="math inline">\(t\)</span> (i.e. <span class="math inline">\(\gamma_X(t+h,t)=\gamma_X(h,0)=Cov(X_h, X_0)\)</span>)</p>
<p><strong><u>Def</u></strong>. For strongly stationary, we require that the joint distribution of <span class="math inline">\(\{X_s\}_{s=0, 1, ..., n}\)</span> is the same as <span class="math inline">\(\{X_{t+s}\}_{s=0, 1, ..., n}\)</span></p>
<p>We define <span class="math inline">\(\gamma_X(h,0)=\gamma_X(h)\)</span> is the <u>auto-covariance</u> function of a stationary series of lag <span class="math inline">\(h\)</span>.</p>
<p>We define <span class="math inline">\(\rho_X(h)\)</span> is the auto-correlation function of lag <span class="math inline">\(h\)</span> and <span class="math inline">\(\rho_X(h)=\frac{\gamma_X(h)}{\gamma_X(0)}=Cor(X_{t+h},X_t)\)</span></p>
<!--Lecture 3-->
<h5 id="useful-identity"><strong>Useful identity</strong></h5>
<p>If <span class="math inline">\(E(X^2) &lt; \infty, E(Y^2) &lt; \infty, E(Z^2) &lt; \infty\)</span> and <span class="math inline">\(a,b,c\)</span> are <u>real</u> constants, then <span class="math inline">\(Cov(aX+bY+c, Z) = aCov(X,Z) + bCov(Y,Z)\)</span>.</p>
<h4 id="example-1-iid-noise"><strong>Example 1: iid noise</strong></h4>
<p><span class="math inline">\(X_t \overset{\text{iid}}{\sim}N(0, \sigma^2)\)</span></p>
<p>By definition we have <span class="math inline">\(E(X_t)=0\)</span>. If <span class="math inline">\(E(X^2) =\sigma^2 &lt; \infty\)</span>, then <span class="math inline">\(\gamma_X(h)=Cov(X_{t+h}, X_t) = \begin{cases} \sigma^2, \text{ if } h=0 \\ 0, \forall h \neq 0 \text{ by independence} \end{cases}\)</span></p>
<p>Therefore iid noise process is weakly stationary.</p>
<h5 id="example-2-white-noise-process"><strong>Example 2: White Noise Process</strong></h5>
<p>If <span class="math inline">\(\{X_t\}\)</span> is a sequence of uncorrelated random variables with <span class="math inline">\(E(X_t)=0, Var(X_t)=\sigma^2 &lt; \infty, \gamma_X(h)=0 \quad \forall h\neq 0\)</span>, then we refer to it as white noise.</p>
<p>Note that iid noise is white noise, but white noise is not necessarily iid noise.</p>
<h5 id="example-3"><strong>Example 3</strong></h5>
<p>Suppose <span class="math inline">\(\{W_t\}\)</span> and <span class="math inline">\(\{Z_t\}\)</span> are iid sequences, and <span class="math inline">\(\{W_t\} \bot \{Z_t\}\)</span>.</p>
<p>Let <span class="math inline">\(\{W_t\}\)</span> follows a Bernoulli distribution, where <span class="math inline">\(Pr(W_i=0)=Pr(W_i=1)=1/2\)</span>.</p>
<p>Let <span class="math inline">\(\{Z_t\}\)</span> follows a transformed Bernoulli distribution, where <span class="math inline">\(Pr(W_i=-1)=Pr(W_i=1)=1/2\)</span>.</p>
<p>Set <span class="math inline">\(X_t=W_t(1-W_{t-1})Z_t\)</span>, and we have the value table of <span class="math inline">\(X_t\)</span> as follows:</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(W_{t-1}\)</span></th>
<th><span class="math inline">\(W_t\)</span></th>
<th><span class="math inline">\(X_t\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td><span class="math inline">\(Z_t\)</span></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(E(X_t)=E(W_t)E(W_{t-1})E(Z_t)=\frac{1}{2} \times \frac{1}{2} \times 0 = 0\)</span></p>
<p>When calculating covariance, there are two cases:</p>
<ol type="1">
<li><span class="math inline">\(h=0\)</span></li>
</ol>
<p><span class="math inline">\(Cov(X_t,X_{t+h})=E(X_t X_{t+h})=E(W_t^2(1-W_{t-1})^2Z_t^2)\\=E(W_t^2)E((1-W_{t-1})^2)E(Z_t^2)=\frac{1}{2} \times \frac{1}{2} \times 1 = \frac{1}{4}\)</span></p>
<ol start="2" type="1">
<li><span class="math inline">\(h\neq 0\)</span></li>
</ol>
<p><span class="math inline">\(Cov(X_t,X_{t+h})=E(X_t X_{t+h})=E(W_t(1-W_{t-1})Z_tW_{t+h}(1-W_{t+h-1})Z_{t+h})\\=E(W_t)E((1-W_{t-1}))E(Z_t)E(W_{t+h})E((1-W_{t+h-1}))E(Z_{t+h})=0\)</span></p>
<p>Therefore, <span class="math inline">\(X_t\)</span> is a white noise process.</p>
<p>Note that <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t-1}\)</span> are dependent but not correlated.</p>
<h5 id="example-4-random-walk"><strong>Example 4: Random walk</strong></h5>
<p>Let <span class="math inline">\(\{X_t\}\)</span> be iid noise, and <span class="math inline">\(S_t = X_1 + ... + X_t = \sum^t_{i=1} X_i\)</span>.</p>
<p>We have <span class="math inline">\(E(S_t)=0\)</span>, and <span class="math inline">\(Var(S_t)=t\sigma^2\)</span>.</p>
<p><span class="math inline">\(Cov(S_{t+h}, S_t)=Cov(S_t+[X_{t+1}+...+X_{t+h}],S_t) \\= Cov(S_t,S_t) + Cov(X_{t+1}+...+X_{t+h},S_t) = t\sigma^2 + 0\)</span></p>
<p>Therefore, random walk is not stationary.</p>
<h5 id="example-5-first-order-moving-average-process-ma1"><strong>Example 5: First order moving average process (MA(1))</strong></h5>
<p>Let <span class="math inline">\(Z_t \sim WN(0, \sigma^2)\)</span>. Let <span class="math inline">\(X_t=Z_t + \theta Z_{t-1},\quad t=0,\pm1,\pm2,...\)</span> where <span class="math inline">\(\theta\)</span> is a real-valued constant.</p>
<p>(Graphical representation will be added later)</p>
<p>We have <span class="math inline">\(E(X_t)=E(Z_t) + \theta E(Z_{t-1}) = 0\)</span>.</p>
<p><span class="math inline">\(Var(X_t) = E(X_t^2) = E((Z_t + \theta Z_{t-1})^2) \\=E(Z_t^2) + 2\theta E(Z_tZ_{t-1}) + \theta^2E(Z_{t-1}^2) = (1+\theta^2)\sigma^2\)</span></p>
<p>When calculating covariance, there are three cases:</p>
<ol type="1">
<li><span class="math inline">\(h=0\)</span></li>
</ol>
<p><span class="math inline">\(\gamma_X(t+h,t)=E(X_{t+h}X_t)=E(X_t^2)=(1+\theta^2)\sigma^2\)</span></p>
<ol start="2" type="1">
<li><span class="math inline">\(h=\pm1\)</span></li>
</ol>
<p><span class="math inline">\(\gamma_X(t+h,t)=E(X_{t+h}X_t)=E((Z_{t+1}+\theta Z_t)(Z_t+\theta Z_{t-1}))\\=E(Z_{t+1}Z_t)+\theta E(Z_t^2)+\theta E(Z_{t+1}Z_{t-1})+\theta^2 E(Z_tZ_{t-1})=\theta\sigma^2\)</span></p>
<ol start="3" type="1">
<li><span class="math inline">\(|h|&gt;1\)</span></li>
</ol>
<p><span class="math inline">\(\gamma_X(t+h,t)=E(X_{t+h}X_t)=E((Z_{t+h}+\theta Z_{t+h-1})(Z_t+\theta Z_{t-1}))=0\)</span> becase <span class="math inline">\(t\neq t-1 \neq t+h \neq t+h-1\)</span> if <span class="math inline">\(|h|&gt;1\)</span></p>
<p>Therefore, MA(1) is stationary, and <span class="math inline">\(\rho_X(h)=\begin{cases} 1, \quad h=0 \\ \frac{\theta}{1+\theta^2}, \quad h=\pm1 \\ 0, \quad |h|&gt;1 \end{cases}\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhenyuan Ma"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhenyuan Ma</p>
  <div class="site-description" itemprop="description">Master student in McGill University 
</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ZhenyuanMa" title="GitHub → https://github.com/ZhenyuanMa" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhenyuan.ma@mail.mcgill.ca" title="E-Mail → mailto:zhenyuan.ma@mail.mcgill.ca" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhenyuan Ma</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
